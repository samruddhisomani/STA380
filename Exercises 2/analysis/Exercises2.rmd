---
title: "Exercises 2"
author: "Samruddhi Somani"
date: "August 19, 2015"
output:
  md_document:
    variant: markdown_github
---
#Question 1

###Question: What airports are the worst to fly in/out of?

__For this exercise, we will define the worst airport as the airport with the longest delays.__

Load libraries.
```{r,include=FALSE}
library(RSocrata)
library(plyr)
library(ggplot2)
library(maps)
library(cowplot)
```

Read in airport data file and longitude/latitude data. Process the data to speed up computations. 

```{r}
airport<-read.csv("https://raw.githubusercontent.com/jgscott/STA380/master/data/ABIA.csv", header=TRUE)

longlat<-read.socrata ('https://opendata.socrata.com/dataset/Airport-Codes-mapped-to-Latitude-Longitude-in-the-/rxrh-4cxm')

attach(airport)

airportkeep<-c("UniqueCarrier","ArrDelay","DepDelay","Origin","Dest","Cancelled")

airport<-airport[airportkeep]
#Drop variables that are irrelevant for this analysis.
airport$ArrDelay[is.na(airport$ArrDelay)]=0 #Replace NA's (missing delays) with zeros. Assume NA means no delay.

airport$DepDelay[is.na(airport$DepDelay)]=0 #Replace NA's (missing delays) with zeros. Assume NA means no delay.

ll<-subset(longlat, locationID%in%airport$Origin | locationID%in%airport$Dest) #keep the airport codes that are in my dataset

usamap<-map_data("usa") #importing USA map
```

First, create dummy variables to divide the flights into two sets: those departing from Austin and those arriving in Austin. We expect these to have different patterns of delays.

```{r}
airport$departing<-"Departing from Austin"
airport$departing[airport$Dest=="AUS"]<-"Arriving into Austin"
airport$departing<-as.factor(airport$departing)
```

Examine the distribution of arrival and departure delays, split by whether flights are arriving into Austin or departing from Austin. We will ignore outliers by zooming in to the area around zero. 

We see arrivals have a much wider spread, both for flights departing from and arriving to Austin: More flights arrive earlier than expected and later than expected as opposed to departing earlier than expected and later than expected. 

Both the arrival delays and the departure delays have similar enough distributions for flights into Austin and out of Austin that we can safely consider them together rather than separately.

```{r,dev="svg",fig.height=8,fig.width=15}
ggarrivalin<-ggplot(data=subset(airport,departing=="Arriving into Austin"))+geom_histogram(aes(x=ArrDelay), binwidth=10)+ coord_cartesian(xlim = c(-50,100),ylim=c(0,30000)) + labs(title="Arrival Delays for Flights\nArriving into Austin", x="Arrival Delay in Minutes", y="Number of Flights")

ggarrivalout<-ggplot(data=subset(airport,departing=="Departing from Austin"))+geom_histogram(aes(x=ArrDelay), binwidth=10)+ coord_cartesian(xlim = c(-50,100),ylim=c(0,30000)) + labs(title="Arrival Delays for Flights\nDeparting from Austin", x="Departure Delay in Minutes", y="Number of Flights")

ggdeparturein<-ggplot(data=subset(airport,departing=="Arriving into Austin"))+geom_histogram(aes(x=DepDelay), binwidth=10)+ coord_cartesian(xlim = c(-50,100),ylim=c(0,30000)) + labs(title="Departure Delays for Flights\nArriving into Austin", x="Arrival Delay in Minutes", y="Number of Flights")

ggdepartureout<-ggplot(data=subset(airport,departing=="Departing from Austin"))+geom_histogram(aes(x=DepDelay), binwidth=10)+ coord_cartesian(xlim = c(-50,100),ylim=c(0,30000)) + labs(title="Departure Delays for Flights\nDeparting from Austin", x="Departure Delay in Minutes", y="Number of Flights")

g<-plot_grid(ggarrivalin, ggarrivalout, ggdeparturein, ggdepartureout, ncol=2)

ggdraw(add_sub(g,label="Arrival delays have wider spreads,\n both flying into Austin and flying out.",x=.5,y=.5,vpadding=grid::unit(2,"lines"),fontface="bold",size=15))
```


Calculate average arrival and departure delays by airport.

```{r}
airportagg<-ddply(airport,.(Origin), summarize, AvgArrDelay=mean(ArrDelay), AvgDepDelay=mean(DepDelay))
airportagg<-merge(airportagg, ll, by.x="Origin", by.y="locationID")
```

Create graphs. The worst departure delays overall (both for flights arriving at Austin and departing from Austin) are in the mid-Atlantic region, with TYS (in Knoxville) being a clear outlier. We see similar patterns for arrival delays. TYS is still by far the worst, and the mid-Atlantic region still tends higher than everywhere else.

```{r,dev="svg",fig.height=8,fig.width=24}
airportagg$Longitude=-(airportagg$Longitude) #uniformizing longitude between dataset and map

ggdep<-ggplot(airportagg) + geom_map(data=usamap, map = usamap, aes(map_id=region,x=long,y=lat), fill="white", color="black") + geom_point(aes(x=Longitude,y=Latitude,size=AvgDepDelay),alpha=.5,color="blue")+ggtitle(paste0(airportagg$Origin[which.max(airportagg$AvgDepDelay)], " has the worst departure delays: ", round(airportagg$AvgDepDelay[which.max(airportagg$AvgDepDelay)],0), " minutes on average")) + scale_size_continuous("Minutes",breaks=c(-25,0,20,40,60), labels=c(-25,0,20,40,60), limits=c(-25,100), range=c(1,10))+theme(axis.line=element_blank(), axis.text=element_blank(),axis.ticks=element_blank(), axis.title=element_blank())

ggarr<-ggplot(airportagg) + geom_map(data=usamap, map = usamap, aes(map_id=region,x=long,y=lat), fill="white", color="black") + geom_point(aes(x=Longitude,y=Latitude,size=AvgArrDelay),alpha=.5,color="blue")+ggtitle(paste0(airportagg$Origin[which.max(airportagg$AvgArrDelay)], " has the worst arrival delays: ", round(airportagg$AvgArrDelay[which.max(airportagg$AvgArrDelay)],0), " minutes on average"))+ scale_size_continuous("Minutes",breaks=c(-25,0,20,40,60), labels=c(-25,0,20,40,60), limits=c(-25,100), range=c(1,10))+theme(axis.line=element_blank(), axis.text=element_blank(),axis.ticks=element_blank(), axis.title=element_blank())

q<-plot_grid(ggdep,ggarr)

ggdraw(add_sub(q,label="Avoid TYS if possible",x=.5,y=.5,vpadding=grid::unit(1,"lines"),fontface="bold",size=15))

```

While we have seen the results aggregated over all flights, we may also want to consider only what happends when a flight is in fact delayed. In other words, which airport most quickly resolves delays? We filter by only departure delays so we retain all the flights that made up for the delay. TYS is still the worst, but many other big airports on the East Coast (as well as OKC) take about an hour to resolve departure delays. Southwestern airports only take about 20 minutes to resolve delays.

```{r,dev="svg",fig.height=8,fig.width=24}
airport_delays<-subset(airport,DepDelay>0)

airportagg<-ddply(airport_delays,.(Origin), summarize, AvgArrDelay=mean(ArrDelay), AvgDepDelay=mean(DepDelay))
airportagg<-merge(airportagg, ll, by.x="Origin", by.y="locationID")

airportagg$Longitude=-(airportagg$Longitude)

ggdep<-ggplot(airportagg) + geom_map(data=usamap, map = usamap, aes(map_id=region,x=long,y=lat), fill="white", color="black") + geom_point(aes(x=Longitude,y=Latitude,size=AvgDepDelay),alpha=.5,color="blue")+ggtitle(paste0(airportagg$Origin[which.max(airportagg$AvgDepDelay)], " has the worst departure delays: ", round(airportagg$AvgDepDelay[which.max(airportagg$AvgDepDelay)],0), " minutes on average")) + scale_size_continuous("Minutes",breaks=c(-25,0,20,40,60), labels=c(-25,0,20,40,60), limits=c(-25,100), range=c(1,10))+theme(axis.line=element_blank(), axis.text=element_blank(),axis.ticks=element_blank(), axis.title=element_blank())

ggarr<-ggplot(airportagg) + geom_map(data=usamap, map = usamap, aes(map_id=region,x=long,y=lat), fill="white", color="black") + geom_point(aes(x=Longitude,y=Latitude,size=AvgArrDelay),alpha=.5,color="blue")+ggtitle(paste0(airportagg$Origin[which.max(airportagg$AvgArrDelay)], " has the worst arrival delays: ", round(airportagg$AvgArrDelay[which.max(airportagg$AvgArrDelay)],0), " minutes on average"))+ scale_size_continuous("Minutes",breaks=c(-25,0,20,40,60), labels=c(-25,0,20,40,60), limits=c(-25,100), range=c(1,10))+theme(axis.line=element_blank(), axis.text=element_blank(),axis.ticks=element_blank(), axis.title=element_blank())

q<-plot_grid(ggdep,ggarr)

ggdraw(add_sub(q,label="Avoid the East Coast. Fly to the Southwest.",x=.5,y=.5,vpadding=grid::unit(1,"lines"),fontface="bold",size=15))
```

###Summary

The worst airport with regards to delays is TYS (Knoxville). However, the East Coast in general has trouble with delays. Southwestern cities handle delays particularly well.

#Question 2

###Creating a Dense Document Term Matrix

Import necessary libraries and source necessary functions. 
```{r, include=FALSE}
library(tm)
library(SnowballC)
library(plyr)
library(maxent)
library(glmnet)
library(caret)

#wrapper function for readPlain
readerPlain = function(fname){
				readPlain(elem=list(content=readLines(fname)), 
							id=fname, language='en') }
```

Import data and create a corpus.
```{r}
author_dirs = Sys.glob('../data/ReutersC50/C50train/*')
file_list = NULL
labels = NULL
for(author in author_dirs) {
	author_name = substring(author, first=29)
	files_to_add = Sys.glob(paste0(author, '/*.txt'))
	file_list = append(file_list, files_to_add)
	labels = append(labels, rep(author_name, length(files_to_add)))
}

all_docs = lapply(file_list, readerPlain) 
names(all_docs) = file_list
names(all_docs) = sub('.txt', '', names(all_docs))
my_corpus = Corpus(VectorSource(all_docs))
names(my_corpus) = labels
```

Preprocess the corpus.
```{r}
my_corpus = tm_map(my_corpus, content_transformer(tolower)) # make everything lowercase
my_corpus = tm_map(my_corpus, content_transformer(removeNumbers)) # remove numbers
my_corpus = tm_map(my_corpus, content_transformer(removePunctuation)) # remove punctuation
my_corpus = tm_map(my_corpus, content_transformer(stripWhitespace)) ## remove excess white-space
my_corpus = tm_map(my_corpus, content_transformer(removeWords), stopwords("en")) #remove basic English stopwords
my_corpus = tm_map(my_corpus, stemDocument) #stem document
```


Create a document term matrix, remove sparse terms, and create a dense matrix. Also, create a vector of training set authors
```{r}
DTM = DocumentTermMatrix(my_corpus) #create a document term matrix
DTM = removeSparseTerms(DTM, 0.996) #remove words that are in ten or fewer documents
X = as.matrix(DTM) #create a dense matrix
authors=rownames(X) #create a vector of training set author names
```

Create a document term matrix and author list for the test set.
```{r}
author_dirs = Sys.glob('../data/ReutersC50/C50test/*')
file_list = NULL
labels = NULL
for(author in author_dirs) {
	author_name = substring(author, first=28)
	files_to_add = Sys.glob(paste0(author, '/*.txt'))
	file_list = append(file_list, files_to_add)
	labels = append(labels, rep(author_name, length(files_to_add)))
}

all_docs = lapply(file_list, readerPlain) 
names(all_docs) = file_list
names(all_docs) = sub('.txt', '', names(all_docs))
my_corpus_test = Corpus(VectorSource(all_docs))
names(my_corpus_test) = labels

my_corpus_test = tm_map(my_corpus_test, content_transformer(tolower)) # make everything lowercase
my_corpus_test = tm_map(my_corpus_test, content_transformer(removeNumbers)) # remove numbers
my_corpus_test = tm_map(my_corpus_test, content_transformer(removePunctuation)) # remove punctuation
my_corpus_test = tm_map(my_corpus_test, content_transformer(stripWhitespace)) ## remove excess white-space
my_corpus_test = tm_map(my_corpus_test, content_transformer(removeWords), stopwords("en")) #remove basic English stopwords
my_corpus_test = tm_map(my_corpus_test, stemDocument) #stem document

DTM_test = DocumentTermMatrix(my_corpus_test) #create a document term matrix
DTM_test = removeSparseTerms(DTM_test, 0.996) #remove words that are in ten or fewer documents
X_test = as.matrix(DTM_test)
authors_test=rownames(X_test)

```

We need to handle words that are in the test set but not in our training set. We will drop them out of our test set because they do not help our model's predictions. We must similarly handle words that are in the training set but not in the test set so our multiplication works. We will add columns of zeros to the test columns.

```{r}
X_names<-colnames(X)
X_test_names<-colnames(X_test)

drop_list<-vector(length=0) #initialize a vector of words to drop out of test
zero_list<-vector(length=0) #initialize a vector of words to drop out of train

for (stem in X_test_names)  { #find word stems that are in test but not in train
  if (!stem %in% X_names) {
    drop_list<-c(drop_list,stem)
  }
}

X_test_mod<-X_test[,!colnames(X_test) %in% drop_list] #drop words from test that are in test but not in train

for (stem in X_names)  { #find words that are in train but not in test
  if (!stem %in% X_test_names) {
    zero_list<-c(zero_list,stem)
  }
}

#add columns of zeros to test for words that are in train but not in test
zeroes<-matrix(0,dim(X_test)[1],length(zero_list))
colnames(zeroes)<-zero_list
X_test_mod<-cbind(zeroes,X_test_mod)
X_test_mod<-X_test_mod[,order(colnames(X_test_mod))]

```

###Model 1: Naive Bayes

_This model assumes words are independently distributed._

Calculate logged multinomial probability vectors for each author.
```{r}
smooth_count=1/nrow(X)
byauthor=rowsum(X+smooth_count, authors) #sum word counts for individual words by author
w=rowSums(byauthor) #sum total word count by author
w_author=log(byauthor/w) #avoid underflow
```

We multiply the modified test DTM by the transposed modified multinomial probability matrix to arrive at a matrix of documents with Naive Bayes scores per author.
```{r}
nbscores<-X_test_mod%*%t(w_author)
```

We create a comparison matrix of the Naive Bayes prediction (the author with the highest logged probabilities per document) versus the actual author.
```{r}
nbprediction<-colnames(nbscores)[max.col(nbscores)] #highest logged probability
nbcorrect<-as.integer(nbprediction==authors_test) #does prediction match actual?
correct_matrix<-cbind.data.frame(authors_test,nbprediction,nbcorrect) #cbind
```

This model correctly predicts the author about 64% of the time across the entire test set.
```{r}
mean(nbcorrect)
```

###Model 2: Multinomial Logistic Regression Using Principal Components

__Our Naive Bayes model assumes that word counts are not correlated. This model allows us to relax that assumption. Because we have so many more features than observations, we will use principal components to reduce the number of features and thus the variance of our final prediction. We will cross-validate to choose the number of principal components we use to build our model.__

We run principal a principal components and  cross-validate over a sequence of k's (number of principal components) to choose which k to use to build our final model.
```{r}

set.seed(1234)

train<-createDataPartition(y = authors, p=0.5, times=1, list = FALSE) #divide the training set into two stratified datasets for cross-validation purposes

CV_train<-X[train,]
CV_test<-X[-train,]
y_train<-as.factor(authors[train])
y_test<-as.factor(authors[-train])

q<-c(2,10,100,seq(250,1000,250)) #create a sequence of k's over which to cross-validate

mean<-numeric(length=length(q)) #create an empty vector to store the accuracy rates

counter=1 #initialize a counter to ensure the means vector is accurately filled out.

pc<-prcomp(CV_train) #run a principal components analysis on the cross-validation training set.

#calculate the accuracy over the sequence of k's
for (n in q) {
  scores<-pc$x[,1:n]
  glm<-glmnet(y=y_train, x=scores, family="multinomial", alpha=0)
  loading<-pc$rotation[,1:n]
  CV_test_scores<-CV_test%*%loading
  predict<-predict(glm,newx=CV_test_scores,type="class",s=0)
  mean[counter]=mean(as.integer(y_test==predict))
  counter=counter+1
}

CV_error=cbind.data.frame(q,mean)

CV_error[which.max(CV_error$mean),] #find the k with the highest accuracy
```

We see that 500 principal components has the highest accuracy. We will use this to build our full model. 
```{r}
pc<-prcomp(X) #run principal components analysis on the entire training dataset.
X_scores<-pc$x[,1:500] #use the first 500 principal components
glm<-glmnet(y=authors,x=X_scores,family="multinomial",alpha=0) #fit a multinomial logistic regression.
```

We transform our test matrix.
```{r}
loading<-pc$rotation[,1:500] #the loadings of the first 500 principal components
X_test_scores<-X_test_mod%*%loading #transform test matrix
mlrpredict<-predict(glm,newx=X_test_scores,type="class",s=0) #predictions
```

We add these predictions and correctness measures to our correct matrix.
```{r}
mlrcorrect<-as.integer(mlrpredict==authors_test) #does prediction match actual?
correct_matrix<-cbind.data.frame(correct_matrix,mlrpredict,mlrcorrect)
```

This model has an accuracy of 63%, which is similar to our Naive Baye's model.
```{r}
mean(mlrcorrect)
```

###Summary

I prefer the Naive Bayes model. Although PCA multinomial logistic regression and Naive Bayes have similar accuracy scores, Naive Bayes is a simpler, more interpretable model that requires fewer computing resources and less time.

The Naive Bayes model particularly struggles with the following authors. They all have accuracy scores under 50%: Fewer than 50% of their articles are correctly attributed to them.
```{r}
final <- ddply(correct_matrix, .(authors_test), transform, sum.n = length(authors_test))
xtab<-ddply(final, .(authors_test, nbprediction), summarise,number = length(nbprediction), proportion = number / sum.n[1] * 100)
poor.prediction<-xtab[xtab$authors_test==xtab$nbprediction & xtab$proportion<50,]
poor.prediction #number and proportion refer to the number and proportion of predictions for that predicted author and that actual author
```

We further examine these authors to see if particular pairs of authors give our Naive Bayes model particular difficulty. The following pairs are often confused. At least 20% (10 documents) of the actual author's documents are misclassifed as the predicted author's works.
```{r}
xtab[xtab$authors_test %in% poor.prediction$authors_test & xtab$number>10 & xtab$authors_test!=xtab$nbprediction,]
```

The MLR model has trouble with the same authors, but it misclassifies most of them across a range of other authors. Only JaneMaCartney and TanEelyn show a high percentage of documents misclassified as by one particular other author (Heather Scoffield and Aaron Pressman, respectively).
```{r}
final <- ddply(correct_matrix, .(authors_test), transform, sum.n = length(authors_test))

xtab<-ddply(final, .(authors_test, mlrpredict), summarise,number = length(mlrpredict), proportion = number / sum.n[1] * 100)

poor.prediction<-xtab[xtab$authors_test==xtab$mlrpredict & xtab$proportion<50,]

poor.prediction #number and proportion refer to the number and proportion of predictions for that predicted author and that actual author

levels(xtab$mlrpredict)<-levels(xtab$authors_test)

xtab[xtab$authors_test %in% poor.prediction$authors_test & xtab$number>10 & xtab$authors_test!=xtab$mlrpredict,]
```


#Problem 3

```{r, include=FALSE}
library(arules)  
```

Read data files and create transactions object.

```{r, include=FALSE}
groceries<-read.transactions("https://raw.githubusercontent.com/jgscott/STA380/master/data/groceries.txt",format="basket",sep=',')
```

Run apriori algorithm with low support and low confidence: These levels let us inspect different subsets without rerunning the apriori algorithm. 
```{r, include=FALSE}
groceriesrules <- apriori(groceries,parameter=list(support=.001, confidence=.1, maxlen=10))
```

###Support=.01 and Confidence=0.5

These choices mean that rarer purchases and weaker relationships are dropped. At these levels, we see that purchases of whole milk are strongly associated with purchases of produce and other dairy, even after accounting for whole milk's relative abundance in our data.

```{r}
inspect(subset(groceriesrules, subset=support>.01&confidence>.5))
```

###Support>.005 and Confidence>0.2 and Lift>2

These choices mean that rarer purchases will still be dropped, but weaker relationships will be kept. The lift parameter ensures that very weak relationships are still dropped. At these levels, we see that a variety of grocery items--dairy, fruits, and meats--are associated with the purchase of root vegetables. Furthermore, we also see that buying any type of dairy (especially in conjunction with a produce item) is associated with buying another type, a relationship that corroborates our earlier findings.

```{r}
inspect(subset(groceriesrules, subset=support>.01&confidence>0.2&lift>2))
```

###Support>.001 and Confidence>0.9 and Lift>4

These choices mean that rarer purchases will be kept, but weaker relationships will be dropped. Our lift parameter ensures that these relationships are meaningful (unlikely to be a chance result). At these levels, the most interesting relationship is between liquor/wine and beer. If a customer buys liquor and wine, we are ~90% confident that they will also buy beer. We also see many relationships between three-grocery itemsets with other vegetables. Shoppers who are making large grocery trips (as opposed to grabbing one or two items) will usually buy some kind of other vegetable.

```{r}
inspect(subset(groceriesrules, subset=support>.001&confidence>0.9&lift>4))
```

###Support>.001 and Confidence>0.2 and Lift>5

These choices mean that rarer purchases and weaker relationships will be kept but only if the relationship is particularly meaningful. At these levels, we begin to see groups of things frequently bought together. Shoppers who buy two types of alcohol are likely to buy the third as well. Shoppers who buy baking supplies are likely to buy a full set (flour, sugar, eggs, baking powder, etc.). Shoppers who buy lunch items are also likely to buy a full set (white bread, processed cheese, ham, etc.)

```{r}
inspect(subset(groceriesrules, subset=support>.001&confidence>0.2&lift>10))
```
